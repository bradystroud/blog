---
canonicalUrl: "https://bradystroud.dev/blogs/openai-model-cheatsheet"
title: OpenAI Chat Model Cheat Sheet - Decoding the Chaos
date: 2025-04-17T10:00:00.000Z
tags:
  - openai
  - gpt
  - ai
  - cheatsheet
  - naming
coverImage: null
---

Let's be honest: OpenAI's model names are a moving target. Just when you've learned what â€œGPT-4 Turboâ€ means, along comes â€œGPT-4o,â€ â€œo4-mini,â€ or â€œGPT-4.1â€â€”and sometimes, the same model gets renamed or quietly replaced. The pace of releases is relentless, and the branding is anything but intuitive. Even experienced developers and AI enthusiasts find themselves double-checking docs, changelogs, and blog posts just to figure out which model is current, which is best for their use case, and what all those suffixes actually mean.

If you feel like you're always a step behind, you're not alone. This cheat sheet exists because the model zoo keeps growing, the names keep shifting, and the only constant is change. Hopefully, this guide helps you cut through the confusion and pick the right model for your needsâ€”at least until the next round of updates drops!

This is a living document and I've probably made some mistakes while writing it, so feel free to suggest changes or additions. I'll keep it updated as OpenAI rolls out new models and features ğŸš€

---

## OpenAI Chat Model Cheat-Sheet

Legend: **ğŸŸ¢ current & recommended** **ğŸŸ¡ active but being phased out** **ğŸ”´ legacy/deprecated**

| ğŸ†•? | Model                                     | Release-(date)                   | Strengths / Best use-cases                                                         | Trade-offs                                      | Naming note                                                  |
| --- | ----------------------------------------- | -------------------------------- | ---------------------------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------ |
| ğŸŸ¢  | **GPT-4.1**                               | 14-Apr-2025                      | Flagship long-context (1 M tokens), top-tier coding & RAG                          | Costliest text-only model                       | â€œ4.1â€-= incremental rev over 4                               |
| ğŸŸ¢  | **GPT-4.1-mini/nano**                     | 14-Apr-2025                      | ~80-90 % of 4.1 accuracy at lower price/latency                                    | Slightly less reasoning depth                   | â€œmini / nanoâ€ denote distillations                           |
| ğŸŸ¢  | **GPT-4o**                                | 13-May-2024                      | Native text-+-image-+-audio (â€œomniâ€) real-time chat                                | Pricier than 4o-mini                            | â€œoâ€-= **o**mni multimodal                                    |
| ğŸŸ¢  | **GPT-4o-mini**                           | Jul-2024                         | Cheaper, faster multimodal                                                         | Lower accuracy vs 4o                            | Same naming rule                                             |
| ğŸŸ¢  | **o4-mini/o4-mini-high**                  | 16-Apr-2025                      | Efficient reasoning, image-aware; â€œhighâ€ spends more tokens for better reliability | Smaller context than 4.1; less brute IQ than o3 | â€œminiâ€-= size; â€œhighâ€-= higher _reasoning_effort_ setting    |
| ğŸŸ¢  | **o3**                                    | 16-Apr-2025                      | Deep step-by-step reasoning, code, math, tool use                                  | Slower & pricier than o4-mini                   | â€œo-seriesâ€ = optimized reasoning line; number is the 3rd gen |
| ğŸŸ¢  | **o3-mini-/-o3-mini-high**                | Feb-2025 (mini), Mar-2025 (high) | Very cheap STEM reasoning; â€œhighâ€ = extra depth                                    | Outclassed by o4-mini on most tasks             | Legacy small variant                                         |
| ğŸŸ¢  | **GPT-3.5-Turbo**                         | 30-Nov-2022                      | Budget workhorse for text                                                          | Reasoning weaker than 4-line                    | â€œTurboâ€-= cost/latency optimized snapshot                    |
| ğŸŸ¡  | **GPT-4 Turbo**                           | Nov-2023                         | 128 K context text/vision                                                          | Sunset mid-2025                                 | â€œTurboâ€ as above                                             |
| ğŸŸ¡  | **GPT-3.5-(base)** (text-davinci-002/003) | 15-Mar-2022 â†’ 28-Nov-2022        | Early RLHF model; ChatGPT v1                                                       | Small context, dated knowledge                  | â€œ3.5â€ signaled RLHF-tuned step between 3 &-4                 |
| ğŸ”´  | **GPT-4**                                 | 14-Mar-2023                      | High-quality text; vision via separate endpoint                                    | Retired from ChatGPT 30-Apr-2025                | Plain version number                                         |
| ğŸ”´  | **GPT-4.5-preview**                       | Feb-2025                         | Bridge model while 4o rolled out                                                   | API removal slated 14-Jul-2025                  | â€œ.5â€-= half-step; â€œpreviewâ€-= experimental                   |
| ğŸ”´  | **GPT-3**                                 | Jun-2020                         | Historic foundation LLM                                                            | Lags on reasoning, cut-off-2020                 | Generation number only                                       |
| ğŸ”´  | **GPT-2**                                 | Feb-2019                         | Historic: text generation demo                                                     | Small context, safety gaps                      | Generation number only                                       |
| ğŸ”´  | **GPT-1**                                 | Jun-2018                         | Proof-of-concept                                                                   | 117 M params, research only                     | First use of â€œGPTâ€                                           |

---

### Decoding the names

- **GPT**-= _Generative Pre-trained Transformer_.
- **Major number** (1,-2,-3,-4,-4.1â€¦) â†’ new architecture/train run.
- **.5 / .1** â†’ mid-cycle fine-tune refresh.
- **o** â†’ **o**mni (multimodal I/O).
- **o-series without â€œGPTâ€** (o1,-o3,-o4â€¦) â†’ separate reasoning line; letters are branding, numbers track generations.
- **mini / nano / high** â†’ trade speed/cost vs accuracy.
- **Turbo** â†’ snapshot tuned for throughput/cost.
- **-MMDD** (e.g., `gpt-4-0613`) â†’ training-snapshot date.

---

Feel free to tweak columns, reorder rows, or add more models as OpenAI's zoo keeps growing ğŸ¦
