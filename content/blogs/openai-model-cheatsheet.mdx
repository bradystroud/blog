---
canonicalUrl: "https://bradystroud.dev/blogs/openai-model-cheatsheet"
title: OpenAI Chat Model Cheat Sheet - Decoding the Chaos
date: 2025-04-17T10:00:00.000Z
tags:
  - openai
  - gpt
  - ai
  - cheatsheet
  - naming
coverImage: null
---

OpenAI model naming has been confusing from the start. The names are very confusing (e.g. o4, 4o), and new versions drop constantly, and it's hard to keep track of what model is the best for what job.

This guide is my attempt to fix that â€” a quick reference I can use whenever I need to choose the right model for a job.

This is a living document and I've probably made some mistakes while writing it, so feel free to suggest changes or additions. I'll keep it updated as OpenAI rolls out new models and features ğŸš€
If you see something missing, or a mistake, please [make a PR](https://github.com/bradystroud/blog/edit/main/content/blogs/openai-model-cheatsheet.mdx)! this page is stored as markdown in GitHub using [TinaCMS ğŸ¦™](https://tina.io)


---

## OpenAI Chat Model Cheat-Sheet

Legend: **ğŸŸ¢ current & recommended** **ğŸŸ¡ active but being phased out** **ğŸ”´ legacy/deprecated**

| ğŸ†•? | Model                                     | Release-(date)                   | Strengths / Best use-cases                                                         | Trade-offs                                      | Naming note                                                  |
| --- | ----------------------------------------- | -------------------------------- | ---------------------------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------ |
| ğŸŸ¢  | **GPT-4.1**                               | 14-Apr-2025                      | Flagship long-context (1 M tokens), top-tier coding & RAG                          | Costliest text-only model                       | â€œ4.1â€-= incremental rev over 4                               |
| ğŸŸ¢  | **GPT-4.1-mini/nano**                     | 14-Apr-2025                      | ~80-90 % of 4.1 accuracy at lower price/latency                                    | Slightly less reasoning depth                   | â€œmini / nanoâ€ denote distillations                           |
| ğŸŸ¢  | **GPT-4o**                                | 13-May-2024                      | Native text-+-image-+-audio (â€œomniâ€) real-time chat                                | Pricier than 4o-mini                            | â€œoâ€-= **o**mni multimodal                                    |
| ğŸŸ¢  | **GPT-4o-mini**                           | Jul-2024                         | Cheaper, faster multimodal                                                         | Lower accuracy vs 4o                            | Same naming rule                                             |
| ğŸŸ¢  | **o4-mini/o4-mini-high**                  | 16-Apr-2025                      | Efficient reasoning, image-aware; â€œhighâ€ spends more tokens for better reliability | Smaller context than 4.1; less brute IQ than o3 | â€œminiâ€-= size; â€œhighâ€-= higher _reasoning_effort_ setting    |
| ğŸŸ¢  | **o3**                                    | 16-Apr-2025                      | Deep step-by-step reasoning, code, math, tool use                                  | Slower & pricier than o4-mini                   | â€œo-seriesâ€ = optimized reasoning line; number is the 3rd gen |
| ğŸŸ¢  | **o3-mini-/-o3-mini-high**                | Feb-2025 (mini), Mar-2025 (high) | Very cheap STEM reasoning; â€œhighâ€ = extra depth                                    | Outclassed by o4-mini on most tasks             | Legacy small variant                                         |
| ğŸŸ¢  | **GPT-3.5-Turbo**                         | 30-Nov-2022                      | Budget workhorse for text                                                          | Reasoning weaker than 4-line                    | â€œTurboâ€-= cost/latency optimized snapshot                    |
| ğŸŸ¡  | **GPT-4 Turbo**                           | Nov-2023                         | 128 K context text/vision                                                          | Sunset mid-2025                                 | â€œTurboâ€ as above                                             |
| ğŸŸ¡  | **GPT-3.5-(base)** (text-davinci-002/003) | 15-Mar-2022 â†’ 28-Nov-2022        | Early RLHF model; ChatGPT v1                                                       | Small context, dated knowledge                  | â€œ3.5â€ signaled RLHF-tuned step between 3 &-4                 |
| ğŸ”´  | **GPT-4**                                 | 14-Mar-2023                      | High-quality text; vision via separate endpoint                                    | Retired from ChatGPT 30-Apr-2025                | Plain version number                                         |
| ğŸ”´  | **GPT-4.5-preview**                       | Feb-2025                         | Bridge model while 4o rolled out                                                   | API removal slated 14-Jul-2025                  | â€œ.5â€-= half-step; â€œpreviewâ€-= experimental                   |
| ğŸ”´  | **GPT-3**                                 | Jun-2020                         | Historic foundation LLM                                                            | Lags on reasoning, cut-off-2020                 | Generation number only                                       |
| ğŸ”´  | **GPT-2**                                 | Feb-2019                         | Historic: text generation demo                                                     | Small context, safety gaps                      | Generation number only                                       |
| ğŸ”´  | **GPT-1**                                 | Jun-2018                         | Proof-of-concept                                                                   | 117 M params, research only                     | First use of â€œGPTâ€                                           |

---

### Decoding the names

- **GPT**-= _Generative Pre-trained Transformer_.
- **Major number** (1,-2,-3,-4,-4.1â€¦) â†’ new architecture/train run.
- **.5 / .1** â†’ mid-cycle fine-tune refresh.
- **o** â†’ **o**mni (multimodal I/O).
- **o-series without â€œGPTâ€** (o1,-o3,-o4â€¦) â†’ separate reasoning line; letters are branding, numbers track generations.
- **mini / nano / high** â†’ trade speed/cost vs accuracy.
- **Turbo** â†’ snapshot tuned for throughput/cost.
- **-MMDD** (e.g., `gpt-4-0613`) â†’ training-snapshot date.

---

Feel free to tweak columns, reorder rows, or add more models as OpenAI's zoo keeps growing ğŸ¦
